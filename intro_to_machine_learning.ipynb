{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "## by Lucas Aledi\n",
    "#### based on [Udacity's Data Analyst Nanodegree extracurricular](https://www.udacity.com/course/data-analyst-nanodegree--nd002?utm_source=gsem_brand&utm_medium=ads_n&utm_campaign=8305564265_c&utm_term=89468963430&utm_keyword=udacity%20data%20analyst_e&gclid=CjwKCAiA6aSABhApEiwA6Cbm_-5FeQPY08ttIzuVyM-8dod7Mjd3kDrgRi3U-WVK2nOTPLOWkCX9sBoC4h0QAvD_BwE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=#table>Table of contents</a>\n",
    "#### <a href=#lesson1>Lesson 1 - Welcome to Machine Learning</a>\n",
    "#### <a href=#lesson2>Lesson 2 - Naive Bayes</a>\n",
    "#### <a href=#lesson3>Lesson 3 - SVM</a>\n",
    "#### <a href=#lesson4>Lesson 4 - Decision Trees</a>\n",
    "#### <a href=#lesson5>Lesson 5 - Choose Your Own Algorithm</a>\n",
    "#### <a href=#lesson6>Lesson 6 - Datasets and Questions</a>\n",
    "#### <a href=#lesson7>Lesson 7 - Regressions</a>\n",
    "#### <a href=#lesson8>Lesson 8 - Outliers</a>\n",
    "#### <a href=#lesson9>Lesson 9 - Clustering</a>\n",
    "#### <a href=#lesson10>Lesson 10 - Feature Scaling</a>\n",
    "#### <a href=#lesson11>Lesson 11 - Text Learning</a>\n",
    "#### <a href=#lesson12>Lesson 12 - Feature Selection</a>\n",
    "#### <a href=#lesson13>Lesson 13 - PCA</a>\n",
    "#### <a href=#lesson14>Lesson 14 - Validation</a>\n",
    "#### <a href=#lesson15>Lesson 15 - Evaluation Metrics</a>\n",
    "#### <a href=#lesson16>Lesson 16 - Tying It All Together</a>\n",
    "#### <a href=#references>References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson1 >Lesson 1 - Welcome to Machine Learning</a>\n",
    "\n",
    "Topics to add a quick summary (pre-requisites):\n",
    "- Linear Regression:\n",
    "\n",
    "Many data sets have an approximately linear relationship between variables. In cases like these, we can predict one variable using a known value for another using a [**best-fit line**](https://www.investopedia.com/terms/l/line-of-best-fit.asp), a line on the form $y= mx + b$ that follows the trends in the data as closesly as possible.\n",
    "<img src=\"img/img1.png\" width=300 height=300 />\n",
    "Here, $x$ is called the **predictor variable** and $y$ is often called the **response variable**.\n",
    "\n",
    "Of course, linear regression is not limited to just one predictor variable. The key concept behind it is the idea that a change in one or more predictor variables will produce a linear change in the response variable.\n",
    "\n",
    "At its core, ML is about taking in information and expanding on it, so it's natural that techniques from statistics play an important role in ML. It is possible to use statistical techniques to find a best-fit line, by first calculating five values about our data. If we represent our data set as a collection of points on a scatter plot, these values are the [means of $x$ and $y$](https://en.wikipedia.org/wiki/Mean), the [standard deviations of $x$ and $y$](https://en.wikipedia.org/wiki/Standard_deviation), and the [correlation coefficient](https://en.wikipedia.org/wiki/Correlation_coefficient).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson2>Lesson 2 - Naive Bayes</a>\n",
    "\n",
    "We shall start taking about **supervised classification**. For instance, self-driving cars are one big supervised classification problem. \n",
    "\n",
    "**Supervised classification** means that we start with a bunch of examples to which we know the correct answer. In other words, we train the machine by showing what's the right behaviour. This, in some sense, emulates the way humans learn: give them a lot of examples and they will eventually start figuring out what's going on.\n",
    "\n",
    "In this lesson we gonna se if we can make a car go fast or slow in specific moments using *supervised learning*. We'll look into a terrain classification problem\n",
    "\n",
    "**Acerous v Non-acerous**<br>\n",
    "Is a horse acerous or non-acerous?\n",
    "\n",
    "When looking at a new example, we have to figure out which parts of it to be paying attention to. Those are called **features**. So, when classifying animals, for example, a feature might be the color, the number of legs or whether it has horns and/or antlers. So, acerous are animals lacking horns and/or antlers and, thus, a horse would fit into it. \n",
    "\n",
    "That is, in general, what machine learning will be doing. We give it a bunch of examples and each one has a number of features, or attributes that we can use to describe it. And if we can pick out the right feature, and it is giving us the right information, we can classify new examples.\n",
    "\n",
    "**Features and Labels**<br>\n",
    "In ML, we often take **features** as input and try to produce (output) **labels**. Let's consider Spotify, for a moment:\n",
    "\n",
    "It takes a song (\"let it go\") and extracts its features (intensity, tempo, genre, gender of the voice, etc). Then, the user processes it into one of two categories: *Like* or *Don't Like*.\n",
    "\n",
    "**Decision Surface and scatter plots**<br>\n",
    "What ML algorithms do is that they take in data and transforms it into a **decision surface** and they tipically lie between different classes.\n",
    "\n",
    "**Naive Bayes**<br>\n",
    "[Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) is a very common algorithm to find a decision surface. We are going to use a python library called [scikit-learn](https://scikit-learn.org/stable/), often abbreviated as **sklearn**, and, of course, *naive bayes*, more specifically [**gaussian naive bayes**](https://scikit-learn.org/stable/modules/naive_bayes.html).\n",
    "\n",
    "So, let's start with a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import relevant sklearn algorithm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# create training points\n",
    "X= np.array([[-1,-1], [-2,-1], [-3,-2], [1,1], [2,1], [3,2]])\n",
    "Y= np.array([1,1,1,2,2,2])\n",
    "\n",
    "# create classifier\n",
    "clf= GaussianNB()\n",
    "\n",
    "# fit/train ML learn the patterns\n",
    "## X are features\n",
    "## Y are lables\n",
    "### this is always true in supervised classification\n",
    "clf.fit(X,Y)\n",
    "\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try GaussianNB deployment on terrain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lesson2_terrain_data/test.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-55cc90871840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprettyPicture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m output_image(\"lesson2_terrain_data/test.png\",\n\u001b[0;32m---> 35\u001b[0;31m              \"png\", open(\"lesson2_terrain_data/test.png\", \"rb\").read())\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lesson2_terrain_data/test.png'"
     ]
    }
   ],
   "source": [
    "\"\"\" Complete the code in ClassifyNB.py with the sklearn\n",
    "    Naive Bayes classifier to classify the terrain data.\n",
    "    \n",
    "    The objective of this exercise is to recreate the decision \n",
    "    boundary found in the lesson video, and make a plot that\n",
    "    visually shows the decision boundary \"\"\"\n",
    "\n",
    "from lesson2_terrain_data.prep_terrain_data import makeTerrainData\n",
    "from lesson2_terrain_data.class_vis import prettyPicture, output_image\n",
    "from lesson2_terrain_data.ClassifyNB import classify\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "### the training data (features_train, labels_train) have both \"fast\" and \"slow\" points mixed\n",
    "### in together--separate them so we can give them different colors in the scatterplot,\n",
    "### and visually identify them\n",
    "grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "\n",
    "\n",
    "# You will need to complete this function imported from the ClassifyNB script.\n",
    "# Be sure to change to that code tab to complete this quiz.\n",
    "clf = classify(features_train, labels_train)\n",
    "\n",
    "\n",
    "\n",
    "### draw the decision boundary with the text points overlaid\n",
    "prettyPicture(clf, features_test, labels_test)\n",
    "output_image(\"lesson2_terrain_data/test.png\",\n",
    "             \"png\", open(\"lesson2_terrain_data/test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson3>Lesson 3 - SVM</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a href=lesson4>Lesson 4 - Decision Trees</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson5>Lesson 5 - Choose Your Own Algorithm</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson6>Lesson 6 - Datasets and Questions</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson7>Lesson 7 - Regressions</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson8>Lesson 8 - Outliers</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson9>Lesson 9 - Clustering</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson10>Lesson 10 - Feature Scaling</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson11>Lesson 11 - Text Learning</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson12>Lesson 12 - Feature Selection</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson13>Lesson 13 - PCA</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson14>Lesson 14 - Validation</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson15>Lesson 15 - Evaluation Metrics</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson16>Lesson 16 - Tying It All Together</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=references>References</a>\n",
    "\n",
    "- [best-fit line](https://www.investopedia.com/terms/l/line-of-best-fit.asp)\n",
    "- [means of $x$ and $y$](https://en.wikipedia.org/wiki/Mean)\n",
    "- [standard deviations of $x$ and $y$](https://en.wikipedia.org/wiki/Standard_deviation)\n",
    "- [correlation coefficient](https://en.wikipedia.org/wiki/Correlation_coefficient)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/)\n",
    "- [gaussian naive bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
