{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "## by Lucas Aledi\n",
    "#### based on [Udacity's Data Analyst Nanodegree extracurricular](https://www.udacity.com/course/data-analyst-nanodegree--nd002?utm_source=gsem_brand&utm_medium=ads_n&utm_campaign=8305564265_c&utm_term=89468963430&utm_keyword=udacity%20data%20analyst_e&gclid=CjwKCAiA6aSABhApEiwA6Cbm_-5FeQPY08ttIzuVyM-8dod7Mjd3kDrgRi3U-WVK2nOTPLOWkCX9sBoC4h0QAvD_BwE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=#table>Table of contents</a>\n",
    "#### <a href=#lesson1>Lesson 1 - Welcome to Machine Learning</a>\n",
    "#### <a href=#lesson2>Lesson 2 - Naive Bayes</a>\n",
    "#### <a href=#lesson3>Lesson 3 - SVM</a>\n",
    "#### <a href=#lesson4>Lesson 4 - Decision Trees</a>\n",
    "#### <a href=#lesson5>Lesson 5 - Choose Your Own Algorithm</a>\n",
    "#### <a href=#lesson6>Lesson 6 - Datasets and Questions</a>\n",
    "#### <a href=#lesson7>Lesson 7 - Regressions</a>\n",
    "#### <a href=#lesson8>Lesson 8 - Outliers</a>\n",
    "#### <a href=#lesson9>Lesson 9 - Clustering</a>\n",
    "#### <a href=#lesson10>Lesson 10 - Feature Scaling</a>\n",
    "#### <a href=#lesson11>Lesson 11 - Text Learning</a>\n",
    "#### <a href=#lesson12>Lesson 12 - Feature Selection</a>\n",
    "#### <a href=#lesson13>Lesson 13 - PCA</a>\n",
    "#### <a href=#lesson14>Lesson 14 - Validation</a>\n",
    "#### <a href=#lesson15>Lesson 15 - Evaluation Metrics</a>\n",
    "#### <a href=#lesson16>Lesson 16 - Tying It All Together</a>\n",
    "#### <a href=#references>References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson1 >Lesson 1 - Welcome to Machine Learning</a>\n",
    "\n",
    "Topics to add a quick summary (pre-requisites):\n",
    "- Linear Regression:\n",
    "\n",
    "Many data sets have an approximately linear relationship between variables. In cases like these, we can predict one variable using a known value for another using a [**best-fit line**](https://www.investopedia.com/terms/l/line-of-best-fit.asp), a line on the form $y= mx + b$ that follows the trends in the data as closesly as possible.\n",
    "<img src=\"img/img1.png\" width=300 height=300 />\n",
    "Here, $x$ is called the **predictor variable** and $y$ is often called the **response variable**.\n",
    "\n",
    "Of course, linear regression is not limited to just one predictor variable. The key concept behind it is the idea that a change in one or more predictor variables will produce a linear change in the response variable.\n",
    "\n",
    "At its core, ML is about taking in information and expanding on it, so it's natural that techniques from statistics play an important role in ML. It is possible to use statistical techniques to find a best-fit line, by first calculating five values about our data. If we represent our data set as a collection of points on a scatter plot, these values are the [means of $x$ and $y$](https://en.wikipedia.org/wiki/Mean), the [standard deviations of $x$ and $y$](https://en.wikipedia.org/wiki/Standard_deviation), and the [correlation coefficient](https://en.wikipedia.org/wiki/Correlation_coefficient).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson2>Lesson 2 - Naive Bayes</a>\n",
    "\n",
    "We shall start taking about **supervised classification**. For instance, self-driving cars are one big supervised classification problem. \n",
    "\n",
    "**Supervised classification** means that we start with a bunch of examples to which we know the correct answer. In other words, we train the machine by showing what's the right behaviour. This, in some sense, emulates the way humans learn: give them a lot of examples and they will eventually start figuring out what's going on.\n",
    "\n",
    "In this lesson we gonna se if we can make a car go fast or slow in specific moments using *supervised learning*. We'll look into a terrain classification problem\n",
    "\n",
    "**Acerous v Non-acerous**<br>\n",
    "Is a horse acerous or non-acerous?\n",
    "\n",
    "When looking at a new example, we have to figure out which parts of it to be paying attention to. Those are called **features**. So, when classifying animals, for example, a feature might be the color, the number of legs or whether it has horns and/or antlers. So, acerous are animals lacking horns and/or antlers and, thus, a horse would fit into it. \n",
    "\n",
    "That is, in general, what machine learning will be doing. We give it a bunch of examples and each one has a number of features, or attributes that we can use to describe it. And if we can pick out the right feature, and it is giving us the right information, we can classify new examples.\n",
    "\n",
    "**Features and Labels**<br>\n",
    "In ML, we often take **features** as input and try to produce (output) **labels**. Let's consider Spotify, for a moment:\n",
    "\n",
    "It takes a song (\"let it go\") and extracts its features (intensity, tempo, genre, gender of the voice, etc). Then, the user processes it into one of two categories: *Like* or *Don't Like*.\n",
    "\n",
    "**Decision Surface and scatter plots**<br>\n",
    "What ML algorithms do is that they take in data and transforms it into a **decision surface** and they tipically lie between different classes.\n",
    "\n",
    "**Naive Bayes**<br>\n",
    "[Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) is a very common algorithm to find a decision surface. We are going to use a python library called [scikit-learn](https://scikit-learn.org/stable/), often abbreviated as **sklearn**, and, of course, *naive bayes*, more specifically [**gaussian naive bayes**](https://scikit-learn.org/stable/modules/naive_bayes.html).\n",
    "\n",
    "So, let's start with a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import relevant sklearn algorithm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# create training points\n",
    "X= np.array([[-1,-1], [-2,-1], [-3,-2], [1,1], [2,1], [3,2]])\n",
    "Y= np.array([1,1,1,2,2,2])\n",
    "\n",
    "# create classifier\n",
    "clf= GaussianNB()\n",
    "\n",
    "# fit/train ML learn the patterns\n",
    "## X are features\n",
    "## Y are lables\n",
    "### this is always true in supervised classification\n",
    "clf.fit(X,Y)\n",
    "\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson3>Lesson 3 - SVM</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a href=lesson4>Lesson 4 - Decision Trees</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson5>Lesson 5 - Choose Your Own Algorithm</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson6>Lesson 6 - Datasets and Questions</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson7>Lesson 7 - Regressions</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson8>Lesson 8 - Outliers</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson9>Lesson 9 - Clustering</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson10>Lesson 10 - Feature Scaling</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson11>Lesson 11 - Text Learning</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson12>Lesson 12 - Feature Selection</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson13>Lesson 13 - PCA</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson14>Lesson 14 - Validation</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson15>Lesson 15 - Evaluation Metrics</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=lesson16>Lesson 16 - Tying It All Together</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#table)\n",
    "# <a id=references>References</a>\n",
    "\n",
    "- [best-fit line](https://www.investopedia.com/terms/l/line-of-best-fit.asp)\n",
    "- [means of $x$ and $y$](https://en.wikipedia.org/wiki/Mean)\n",
    "- [standard deviations of $x$ and $y$](https://en.wikipedia.org/wiki/Standard_deviation)\n",
    "- [correlation coefficient](https://en.wikipedia.org/wiki/Correlation_coefficient)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/)\n",
    "- [gaussian naive bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
